from typing import List, Dict, Any, TypedDict, Literal, Union
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langgraph.graph import StateGraph
from pydantic import BaseModel, Field
import base64
from pathlib import Path
from openai import OpenAI
import os
from medical_agent.utils import call_qwen_vl_api, safe_json_load, end_timer_and_print
from medical_agent.utils import *
from medical_agent.table_format import create_formatted_df, ROW_INDEX
from typing import TypedDict, get_type_hints, Any
from medical_agent.prompts import FILL_IN_FORM_PROMPT, FILLIN_PROMPT_2, FILLIN_PROMPT_3, FILLIN_PROMPT_4, FILLIN_PROMPT_5, REPORT_CLASSIFIER_PROMPT, ULTRASOUND_EXTRACT_PROMPT
import json
import pandas as pd
from medical_agent.gui import show_popup_with_df
from medical_agent.utils import ROOT_DIR
from rapidfuzz import fuzz, process

# Define message types
class Message(TypedDict):
    role: Literal["user", "assistant", "system"]
    content: Union[str, Dict[str, Any]]
    content_type: Literal["text", "image"] = "text"


# Define the system prompt
SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªåŒ»ç–—åŠ©æ‰‹ï¼Œä½ çš„ä¸»è¦ä»»åŠ¡æ˜¯å¸®åŠ©åŒ»ç”Ÿæ•´ç†ç—…ä¾‹ï¼Œè¯Šæ–­æŠ¥å‘Šç­‰æ–‡ä»¶æˆ–å›¾ç‰‡ï¼Œå¹¶å°†å…¶æ•´ç†æˆç»“æ„åŒ–æ•°æ®å­˜å‚¨èµ·æ¥ã€‚å¿…è¦æ—¶ï¼Œä½ ä¹Ÿå¯ä»¥å›ç­”ç”¨æˆ·çš„åŒ»ç–—é—®é¢˜ã€‚å¦‚æœå¯èƒ½ï¼Œåœ¨å›ç­”åŒ»ç–—é—®é¢˜æ—¶è¯·å°½é‡æä¾›æ¥æºã€‚"""

def process_ultrasound_location(location, ocr, qwen, row_index, system_prompt, model_name="qwen-max-0125"):
    """å¤„ç†å•ä¸ªè¶…å£°æµ‹é‡é¡¹ç›®çš„å‡½æ•°ï¼Œç”¨äºå¹¶è¡Œæ‰§è¡Œ"""
    if location not in row_index:
        return None, None, None
        
    ridx = row_index[location]
    
    # åŠ¨æ€ç”Ÿæˆåˆ«åæ˜ å°„è§„åˆ™
    try:
        from config_loader import generate_alias_prompt_section
        dynamic_alias_rules = generate_alias_prompt_section()
    except (ImportError, Exception):
        # ä¸ä½¿ç”¨ä»»ä½•åˆ«åè§„åˆ™ï¼Œè®©AIè¿›è¡Œç²¾ç¡®åŒ¹é…
        dynamic_alias_rules = ""
    
    input_prompt = ULTRASOUND_EXTRACT_PROMPT.format(
        ocr_text=ocr, 
        location=location,
        dynamic_alias_rules=dynamic_alias_rules
    )
    
    # å®ç°é‡è¯•æœºåˆ¶ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
    max_retries = 3
    attempt = 0
    
    while attempt < max_retries:
        try:
            completion = qwen.chat.completions.create(
                model=model_name,
                messages=[
                    {'role': 'system', 'content': system_prompt},
                    {'role': 'user', 'content': input_prompt}
                ]
            )
            text = completion.choices[0].message.content
            tmp = safe_json_load(text)
            return location, ridx, tmp
        except Exception as e:
            attempt += 1
            if attempt < max_retries:
                # æŒ‡æ•°é€€é¿ï¼šç­‰å¾…æ—¶é—´ä¸º 2^attempt ç§’ï¼Œæœ€å¤§ç­‰å¾…32ç§’
                wait_time = min(2 ** attempt, 32)
                import time
                time.sleep(wait_time)
            else:
                print(f"âŒ {location} å¤„ç†å¤±è´¥: {e}")
                return location, ridx, None
    
    return location, ridx, None

def calculate_ea_ratios(formatted_table):
    """
    è‡ªåŠ¨è®¡ç®—äºŒå°–ç“£/ä¸‰å°–ç“£çš„E/Aæ¯”å€¼
    
    Args:
        formatted_table (pd.DataFrame): æ ¼å¼åŒ–çš„è¡¨æ ¼
        
    Returns:
        pd.DataFrame: æ·»åŠ äº†E/Aæ¯”å€¼è®¡ç®—çš„è¡¨æ ¼
    """
    # å®šä¹‰Eå³°å’ŒAå³°çš„æœç´¢æ¨¡å¼
    valve_patterns = {
        "äºŒå°–ç“£": {
            "e_patterns": ["äºŒå°–ç“£E", "MV E", "Mitral E", "äºŒå°–ç“£Eå³°", "MV Eå³°"],
            "a_patterns": ["äºŒå°–ç“£A", "MV A", "Mitral A", "äºŒå°–ç“£Aå³°", "MV Aå³°"],
            "ratio_name": "äºŒå°–ç“£E/Aæ¯”å€¼",
            "ratio_english": "MV E/A ratio",
            "ratio_abbr": "MV E/A"
        },
        "ä¸‰å°–ç“£": {
            "e_patterns": ["ä¸‰å°–ç“£E", "TV E", "Tricuspid E", "ä¸‰å°–ç“£Eå³°", "TV Eå³°"],
            "a_patterns": ["ä¸‰å°–ç“£A", "TV A", "Tricuspid A", "ä¸‰å°–ç“£Aå³°", "TV Aå³°"],
            "ratio_name": "ä¸‰å°–ç“£E/Aæ¯”å€¼",
            "ratio_english": "TV E/A ratio", 
            "ratio_abbr": "TV E/A"
        }
    }
    
    ea_ratios_added = []
    
    for valve_name, patterns in valve_patterns.items():
        # æŸ¥æ‰¾Eå³°å’ŒAå³°çš„æ•°å€¼
        e_value = None
        a_value = None
        e_row_idx = None
        a_row_idx = None
        
        # æœç´¢Eå³°
        for idx, row in formatted_table.iterrows():
            name = str(row['åç§°']).strip()
            value = str(row['æ•°å€¼']).strip()
            
            # æ£€æŸ¥æ˜¯å¦åŒ¹é…Eå³°æ¨¡å¼ä¸”æœ‰æ•°å€¼
            for pattern in patterns["e_patterns"]:
                if pattern in name and value and value != "" and value != "-" and value != "NO":
                    try:
                        e_value = float(value)
                        e_row_idx = idx
                        break
                    except ValueError:
                        continue
            if e_value is not None:
                break
        
        # æœç´¢Aå³°  
        for idx, row in formatted_table.iterrows():
            name = str(row['åç§°']).strip()
            value = str(row['æ•°å€¼']).strip()
            
            # æ£€æŸ¥æ˜¯å¦åŒ¹é…Aå³°æ¨¡å¼ä¸”æœ‰æ•°å€¼
            for pattern in patterns["a_patterns"]:
                if pattern in name and value and value != "" and value != "-" and value != "NO":
                    try:
                        a_value = float(value)
                        a_row_idx = idx
                        break
                    except ValueError:
                        continue
            if a_value is not None:
                break
        
        # å¦‚æœæ‰¾åˆ°äº†å®Œæ•´çš„Eå³°å’ŒAå³°æ•°å€¼ï¼Œè®¡ç®—E/Aæ¯”å€¼
        if e_value is not None and a_value is not None and a_value != 0:
            ea_ratio = round(e_value / a_value, 2)
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨å¯¹åº”çš„E/Aæ¯”å€¼è¡Œ
            ratio_exists = False
            for idx, row in formatted_table.iterrows():
                name = str(row['åç§°']).strip()
                if patterns["ratio_name"] in name or patterns["ratio_abbr"] in name:
                    # æ›´æ–°ç°æœ‰çš„æ¯”å€¼
                    formatted_table.at[idx, 'æ•°å€¼'] = str(ea_ratio)
                    ratio_exists = True
                    break
            
            # å¦‚æœä¸å­˜åœ¨ï¼Œæ·»åŠ æ–°è¡Œ
            if not ratio_exists:
                new_row = {
                    "åç§°": f"{patterns['ratio_name']}({patterns['ratio_abbr']})",
                    "è‹±æ–‡": patterns["ratio_english"],
                    "æ–‘å—ç§ç±»": "",
                    "ç±»å‹": "",
                    "ç—‡çŠ¶": "",
                    "æ•°å€¼": str(ea_ratio),
                    "å•ä½": "",  # E/Aæ¯”å€¼æ— å•ä½
                    "ç‹­çª„ç¨‹åº¦": "",
                    "é—­å¡": "å¦"
                }
                
                # ä½¿ç”¨pd.concatæ·»åŠ æ–°è¡Œåˆ°è¡¨æ ¼å¼€å¤´
                new_row_df = pd.DataFrame([new_row])
                formatted_table = pd.concat([new_row_df, formatted_table], ignore_index=True)
                
                ea_ratios_added.append(f"{patterns['ratio_name']}: {ea_ratio}")
    
    return formatted_table

def separate_value_and_unit(value_str):
    """
    å°†å¸¦å•ä½çš„æ•°å€¼å­—ç¬¦ä¸²åˆ†ç¦»æˆçº¯æ•°å€¼å’Œå•ä½
    
    Args:
        value_str (str): å¸¦å•ä½çš„æ•°å€¼å­—ç¬¦ä¸²ï¼Œå¦‚ "700m/s", "500x300cm", "18mmHg"
    
    Returns:
        tuple: (çº¯æ•°å€¼, å•ä½)
        
    Examples:
        "700m/s" -> ("700", "m/s")
        "500x300cm" -> ("500x300", "cm") 
        "18mmHg" -> ("18", "mmHg")
        "59%" -> ("59", "%")
        "18" -> ("18", "")
    """
    import re
    
    if not value_str or value_str == "-" or value_str == "NO":
        return value_str, ""
    
    value_str = str(value_str).strip()
    
    # å®šä¹‰å¸¸è§çš„åŒ»å­¦å•ä½ï¼ˆæŒ‰é•¿åº¦ä»é•¿åˆ°çŸ­æ’åºï¼Œä¼˜å…ˆåŒ¹é…è¾ƒé•¿çš„å•ä½ï¼‰
    common_units = [
        'mmHg', 'ml/mÂ²', 'cm/s', 'mm/s', 'm/s', 'mmHg/s', 'msec', 
        'cmÂ²', 'mmÂ²', 'mÂ²', 'bpm', 'cm', 'mm', 'm', 'ml', 'kPa', 
        'Hz', 'sec', 'min', '%', 'g', 'kg', 'l'
    ]
    
    # å…ˆå°è¯•æ‰¾åˆ°æœ€é•¿åŒ¹é…çš„å•ä½åç¼€
    matched_unit = ""
    numeric_part = value_str
    
    for unit in common_units:
        # æ£€æŸ¥å­—ç¬¦ä¸²æ˜¯å¦ä»¥è¿™ä¸ªå•ä½ç»“å°¾
        if value_str.lower().endswith(unit.lower()):
            # æå–æ•°å€¼éƒ¨åˆ†ï¼ˆå»æ‰å•ä½åçš„éƒ¨åˆ†ï¼‰
            potential_numeric = value_str[:-len(unit)].strip()
            
            # éªŒè¯æ•°å€¼éƒ¨åˆ†æ˜¯å¦æ˜¯æœ‰æ•ˆçš„æ•°å€¼æ ¼å¼ï¼ˆåŒ…æ‹¬å¤åˆæ ¼å¼å¦‚ 200x160ï¼‰
            # å…è®¸ï¼šæ•°å­—ã€å°æ•°ç‚¹ã€xã€<ã€>ã€ç©ºæ ¼
            if re.match(r'^[0-9]+(?:\.[0-9]+)?(?:\s*[xÃ—]\s*[0-9]+(?:\.[0-9]+)?)*\s*[<>]?\s*$', potential_numeric, re.IGNORECASE):
                matched_unit = unit
                numeric_part = potential_numeric.strip()
                break
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡å‡†å•ä½ï¼Œä½†å­—ç¬¦ä¸²åŒ…å«æ•°å­—ï¼Œå°è¯•åˆ†ç¦»
    if not matched_unit:
        # åŒ¹é…æ¨¡å¼ï¼šæ•°å­—éƒ¨åˆ† + éæ•°å­—éƒ¨åˆ†
        match = re.match(r'^([0-9]+(?:\.[0-9]+)?(?:\s*[xÃ—]\s*[0-9]+(?:\.[0-9]+)?)*\s*[<>]?\s*)(.*?)$', value_str, re.IGNORECASE)
        if match:
            numeric_part = match.group(1).strip()
            unit_part = match.group(2).strip()
            if unit_part:  # å¦‚æœæœ‰å•ä½éƒ¨åˆ†
                matched_unit = unit_part
    
    return numeric_part, matched_unit

class AgentState(TypedDict):
    messages: list
    qwen: Any
    gpt: Any
    formatted_table: Any
    row_index: dict
    image_content: dict
    context: dict
    next: str

# Dynamically create an initial state with sensible defaults
def init_typed_dict(cls: TypedDict):
    hints = get_type_hints(cls)
    default_values = {
        list: [],
        dict: {},
        str: "",
        int: 0,
        float: 0.0,
        bool: False,
    }

    state = {}
    for key, hint in hints.items():
        # handle special case of typing.List, typing.Dict, etc.
        origin = getattr(hint, '__origin__', hint)
        state[key] = default_values.get(origin, None)
    return state


def init_llms(state: AgentState):
    """
    Initialize dual-client setup: OCR client + Medical LLM client
    """
    state = init_typed_dict(AgentState)

    # OCR ä¸“ç”¨å®¢æˆ·ç«¯ (ç»§ç»­ä½¿ç”¨ Qwen-VL)
    state["ocr_client"] = OpenAI(
        # è‹¥æ²¡æœ‰é…ç½®ç¯å¢ƒå˜é‡ï¼Œè¯·ç”¨ç™¾ç‚¼API Keyå°†ä¸‹è¡Œæ›¿æ¢ä¸ºï¼šapi_key="sk-xxx"
        api_key=os.getenv('DASHSCOPE_API_KEY'),
        base_url=os.getenv('DASHSCOPE_BASE_URL', "https://dashscope-intl.aliyuncs.com/compatible-mode/v1"),
    )
    
    # åŒ»ç–—æ¨ç†ä¸“ç”¨å®¢æˆ·ç«¯ (ä½¿ç”¨æœ¬åœ°éƒ¨ç½²çš„ Baichuan M2)
    # å¦‚æœæœ¬åœ°æœåŠ¡ä¸å¯ç”¨ï¼Œä¼šè‡ªåŠ¨fallbackåˆ°OCRå®¢æˆ·ç«¯
    baichuan_base_url = os.getenv('BAICHUAN_BASE_URL', 'http://localhost:8000/v1')
    state["medical_llm"] = OpenAI(
        api_key=os.getenv('BAICHUAN_API_KEY', 'not-needed'),  # æœ¬åœ°éƒ¨ç½²é€šå¸¸ä¸éœ€è¦
        base_url=baichuan_base_url,
    )
    
    # ä¿ç•™åŸæœ‰çš„ qwen å®¢æˆ·ç«¯ä½œä¸ºå¤‡é€‰ (å‘åå…¼å®¹)
    state["qwen"] = state["ocr_client"]  # æŒ‡å‘OCRå®¢æˆ·ç«¯ï¼Œä¿æŒå…¼å®¹æ€§
    
    # ä¿ç•™åŸæœ‰çš„ GPT å®¢æˆ·ç«¯
    state["gpt"] = ChatOpenAI(
        model="gpt-4o",
        max_tokens=1024,
        temperature=0.7
    )

    state['messages'].append({
        "role": "system",
        "content": SYSTEM_PROMPT
    })

    state['formatted_table'] = create_formatted_df()
    from medical_agent.table_format import get_dynamic_row_index
    state['row_index'] = get_dynamic_row_index()

    return state

def init_llms_for_ocr(state: AgentState):
    """
    Initialize OCR-only client for pure OCR tasks
    """
    state = init_typed_dict(AgentState)

    # OCR ä¸“ç”¨å®¢æˆ·ç«¯ (ç»§ç»­ä½¿ç”¨ Qwen-VL)
    state["ocr_client"] = OpenAI(
        # è‹¥æ²¡æœ‰é…ç½®ç¯å¢ƒå˜é‡ï¼Œè¯·ç”¨ç™¾ç‚¼API Keyå°†ä¸‹è¡Œæ›¿æ¢ä¸ºï¼šapi_key="sk-xxx"
        api_key=os.getenv('DASHSCOPE_API_KEY'),
        base_url=os.getenv('DASHSCOPE_BASE_URL', "https://dashscope-intl.aliyuncs.com/compatible-mode/v1"),
    )
    
    # ä¿ç•™åŸæœ‰çš„ qwen å®¢æˆ·ç«¯æŒ‡å‘OCRå®¢æˆ·ç«¯ï¼Œä¿æŒå…¼å®¹æ€§
    state["qwen"] = state["ocr_client"]
    
    # ä¿ç•™åŸæœ‰çš„ GPT å®¢æˆ·ç«¯
    state["gpt"] = ChatOpenAI(
        model="gpt-4o",
        max_tokens=1024,
        temperature=0.7
    )

    state['messages'].append({
        "role": "system",
        "content": SYSTEM_PROMPT
    })

    # æ³¨é‡Šæ‰OCRé˜¶æ®µçš„è¡¨æ ¼åˆ›å»ºï¼Œåªåœ¨æœ€ç»ˆåˆ†ææ—¶åˆ›å»º
    # state['formatted_table'] = create_formatted_df()
    # state['row_index'] = ROW_INDEX

    return state

# Define the response generation node
def create_input_node(state: AgentState):
    """Create a node for handling user input and generating responses."""
    
    # Default values
    default_image_path = os.path.join(ROOT_DIR, "../../data/input_2.jpg")
    default_question = "è¯·åˆ†æè¿™å¼ åŒ»ç–—å›¾åƒå¹¶æä¾›è¯Šæ–­å»ºè®®ã€‚"
    
    image_path = default_image_path
    question = default_question
    
    # Get user input or use defaults
    # DEBUG æ¨¡å¼ä¸‹è·³è¿‡è¾“å…¥
    if os.environ.get('DEBUG', '0') == '0':
        try:
            user_image_path = input("è¯·è¾“å…¥å›¾ç‰‡è·¯å¾„ (ç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤è·¯å¾„): ").strip()
            user_question = input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ (ç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤é—®é¢˜): ").strip()
            if user_image_path:
                image_path = user_image_path
            if user_question:
                question = user_question
        except:
            pass
    
    # Read and encode the image
    try:
        image_path = Path(image_path)
        if not image_path.exists():
            print(f"è­¦å‘Šï¼šæ‰¾ä¸åˆ°å›¾ç‰‡ {image_path}ï¼Œè¯·é‡æ–°è¾“å…¥")
            raise RuntimeError(f"æ‰¾ä¸åˆ°å›¾ç‰‡ {image_path}")
            
        with open(image_path, "rb") as image_file:
            base64_image = base64.b64encode(image_file.read()).decode('utf-8')
            
        # Create image content in the format expected by the API
        image_content = {
            "type": "image_url",
            "image_url": {
                "url": f"data:image/jpeg;base64,{base64_image}"
            }
        }

        state['image_content'] = image_content
        
        # Add the message to state
        state["messages"].append({
            "role": "user",
            "content": [
                image_content,
                {"type": "text", "text": question}
            ],
            "content_type": "image"
        })
        
    except Exception as e:
        print(f"å¤„ç†å›¾ç‰‡æ—¶å‡ºé”™: {e}")
        # If image processing fails, fall back to text-only
        state["messages"].append({
            "role": "user",
            "content": question,
            "content_type": "text"
        })
    
    return state


def ocr_node(state: AgentState):
    """Create a node for OCR using dedicated OCR client."""
    image_content = state['image_content']
    if not image_content:
        print("Warning: There is no image content to process. Skipping OCR.")
        return state

    # Call the OCR API using dedicated OCR client
    messages=[
        {
            "role": "user",
            "content": [
                image_content,
                # ä¸ºä¿è¯è¯†åˆ«æ•ˆæœï¼Œå¦‚æœä½¿ç”¨qwen-vl-ocr ç³»åˆ—æ¨¡å‹ï¼Œ ç›®å‰æ¨¡å‹å†…éƒ¨ä¼šç»Ÿä¸€ä½¿ç”¨"Read all the text in the image."è¿›è¡Œè¯†åˆ«ï¼Œç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬ä¸ä¼šç”Ÿæ•ˆã€‚
                {"type": "text", "text": "Read all the text in the image"},
            ],
        }
    ]

    print("æ­£åœ¨è°ƒç”¨OCRæ¨¡å‹è·å–æ–‡æœ¬æå–ç»“æœ")
    
    # ğŸ” è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥DEBUGç¯å¢ƒå˜é‡
    debug_mode = os.environ.get('DEBUG', '0')
    print(f"ğŸ” DEBUGæ¨¡å¼çŠ¶æ€: DEBUG={debug_mode}")
    
    if debug_mode == '1':
        text = """CTæ£€æŸ¥æŠ¥å‘Šå•
(æ‰«ç æŸ¥çœ‹å›¾åƒ
æ£€æŸ¥å·ï¼š220901
ç”³è¯·ç§‘å®¤ï¼šå†…åˆ†æ³Œç§‘â…¡ç—…åŒº
ç”³è¯·åŒ»ç”Ÿï¼šå¼ ç²
å§“åï¼š ä½é™¢å·ï¼š2022041478 å¹´é¾„ï¼š55å² æ€§åˆ«ï¼šç”· åºŠå·ï¼š5
æ£€æŸ¥é¡¹ç›®ï¼š256æ’å† çŠ¶åŠ¨è„‰CTA
æ£€æŸ¥æ‰€è§ï¼š
å† çŠ¶åŠ¨è„‰å‘ˆå³ä¼˜åŠ¿å‹ã€‚å·¦ä¸»å¹²èµ·æºäºå·¦çª¦ï¼Œå³å† çŠ¶åŠ¨è„‰èµ·æºäºå³çª¦ã€‚
å·¦ä¸»å¹²ç®¡å£å¯è§é’™åŒ–æ–‘å—ï¼Œç®¡è…”è½»å¾®ç‹­çª„çº¦10%ã€‚å·¦å‰é™æ”¯è¿‘æ®µç®¡å£å¯è§é’™åŒ–æ–‘å—ï¼Œç®¡è…”è½»åº¦ç‹­çª„çº¦25%ï¼›ä¸­æ®µç®¡å£å¯è§æ··åˆæ–‘å—ï¼Œç®¡è…”é‡åº¦ç‹­çª„çº¦85%ï¼›è¿œæ®µç®¡å£å¯è§é’™åŒ–æ–‘å—ï¼Œç®¡è…”è½»åº¦ç‹­çª„çº¦25%ã€‚ç¬¬ä¸€ï¼Œç¬¬äºŒå¯¹è§’æ”¯æœªè§æ–‘å—åŠæ˜æ˜¾ç‹­çª„ã€‚å·¦å›æ—‹æ”¯ä¸­è¿œæ®µç®¡å£å¯è§éé’™åŒ–æ–‘å—ï¼Œç®¡è…”è½»åº¦ç‹­çª„çº¦25%ï¼›è¿‘æ®µæœªè§æ–‘å—åŠæ˜æ˜¾ç‹­çª„ã€‚ç¬¬ä¸€ï¼Œç¬¬äºŒé’ç¼˜æ”¯æœªè§æ–‘å—åŠæ˜æ˜¾ç‹­çª„ã€‚ä¸­é—´æ”¯æœªè§æ–‘å—åŠæ˜æ˜¾ç‹­çª„ã€‚
å³å† çŠ¶åŠ¨è„‰è¿‘æ®µç®¡å£å¯è§é’™åŒ–ã€éé’™åŒ–æ–‘å—ï¼Œç®¡è…”è½»åº¦ç‹­çª„çº¦25%ï¼›ä¸­æ®µã€è¿œæ®µæœªè§æ–‘å—åŠæ˜æ˜¾ç‹­çª„ã€‚å³åé™æ”¯æœªè§æ–‘å—åŠæ˜æ˜¾ç‹­çª„ã€‚å·¦å®¤åæ”¯æœªè§æ–‘å—åŠæ˜æ˜¾ç‹­çª„ã€‚
å¿ƒè„å„è…”å®¤ä¸å¤§ï¼Œå¿ƒè‚Œæœªè§å¼‚å¸¸å¯†åº¦å½±ã€‚
å°è±¡ï¼š
å† çŠ¶åŠ¨è„‰CTAï¼š1.å·¦ä¸»å¹²ç®¡å£é’™åŒ–æ–‘å—ï¼Œç®¡è…”è½»å¾®ç‹­çª„ã€‚2.å·¦å‰é™æ”¯è¿‘æ®µç®¡å£é’™åŒ–æ–‘å—ï¼Œç®¡è…”è½»åº¦ç‹­çª„ï¼›ä¸­æ®µç®¡å£æ··åˆæ–‘å—ï¼Œç®¡è…”é‡åº¦ç‹­çª„ï¼›è¿œæ®µç®¡å£é’™åŒ–æ–‘å—ï¼Œç®¡è…”è½»åº¦ç‹­çª„ã€‚3.å·¦å›æ—‹æ”¯ä¸­è¿œæ®µç®¡å£éé’™åŒ–æ–‘å—ï¼Œç®¡è…”è½»åº¦ç‹­çª„ã€‚4.å³å† çŠ¶åŠ¨è„‰è¿‘æ®µç®¡å£é’™åŒ–ã€éé’™åŒ–æ–‘å—ï¼Œç®¡è…”è½»åº¦ç‹­çª„ã€‚
æ£€æŸ¥æ—¥æœŸï¼š2022-09-08 å®¡æ ¸åŒ»å¸ˆï¼š
æŠ¥å‘ŠåŒ»å¸ˆï¼š
æ³¨ï¼š1.æœ¬æŠ¥å‘Šä»…ä¾›ä¸´åºŠç§‘å®¤ç”³è¯·åŒ»ç”Ÿè¯Šæ²»å‚è€ƒï¼
2.äºŒç»´ç é“¾æ¥å›¾åƒï¼Œè¯·å¦¥å–„ä¿å­˜æœ¬æŠ¥å‘Šï¼
æŠ¥å‘Šæ—¶é—´ï¼š2022-09-08 17:19:39
        """
    else:
        # ä½¿ç”¨ä¸“é—¨çš„ OCR å®¢æˆ·ç«¯
        completion = state["ocr_client"].chat.completions.create(
            model="qwen-vl-ocr",
            messages=messages
        )
        text = completion.choices[0].message.content
    
    # ğŸ” è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºOCRæå–ç»“æœçš„å‰200å­—ç¬¦
    print(f"ğŸ” OCRæå–æ–‡æœ¬é¢„è§ˆ: {text[:200]}...")
    print(f"ğŸ” OCRæ–‡æœ¬æ€»é•¿åº¦: {len(text)} å­—ç¬¦")

    state['context']['ocr'] = text
    return state


def get_medical_llm_client(state: AgentState):
    """
    æ™ºèƒ½é€‰æ‹©æ–‡æœ¬ç†è§£å®¢æˆ·ç«¯
    æš‚æ—¶å…¨éƒ¨ä½¿ç”¨Qwenï¼Œä¿è¯ç¨³å®šæ€§
    """
    # ç›´æ¥ä½¿ç”¨Qwenå®¢æˆ·ç«¯ï¼Œç¡®ä¿ç¨³å®šæ€§
    return state["qwen"], "qwen-max-0125"


def fill_form_node(state: AgentState):
    """
    æ™ºèƒ½åˆ†æµç‰ˆæœ¬çš„è¡¨æ ¼å¡«å……èŠ‚ç‚¹
    
    å·¥ä½œæµç¨‹ï¼š
    1. å…ˆè¯†åˆ«æŠ¥å‘Šç±»å‹ï¼ˆCTA æˆ– è¶…å£°ï¼‰
    2. æ ¹æ®æŠ¥å‘Šç±»å‹é€‰æ‹©ç›¸åº”çš„å¤„ç†é€»è¾‘
    3. è¾“å‡ºç»Ÿä¸€çš„æ ¼å¼
    """
    print("ğŸš€ å¼€å§‹æ™ºèƒ½ç»“æ„åŒ–æå–...")
    
    # è·å–åŸºæœ¬æ•°æ®
    ocr = state['context']['ocr']
    formatted_table = state['formatted_table']
    row_index = state['row_index']
    
    # æ™ºèƒ½é€‰æ‹©æ–‡æœ¬ç†è§£å®¢æˆ·ç«¯
    medical_client, medical_model = get_medical_llm_client(state)
    qwen = medical_client  # ä¿æŒå‘åå…¼å®¹
    
    # ğŸ” è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºä½¿ç”¨çš„æ¨¡å‹
    print(f"ğŸ” ä½¿ç”¨çš„AIæ¨¡å‹: {medical_model}")
    print(f"ğŸ” OCRæ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«'å† çŠ¶åŠ¨è„‰': {'å† çŠ¶åŠ¨è„‰' in ocr}")
    print(f"ğŸ” OCRæ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«'è¶…å£°': {'è¶…å£°' in ocr}")
    print(f"ğŸ” OCRæ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«'å¿ƒåŠ¨å›¾': {'å¿ƒåŠ¨å›¾' in ocr}")
    
    # ==============================================================
    # ç¬¬ä¸€æ­¥ï¼šæ™ºèƒ½è¯†åˆ«æŠ¥å‘Šç±»å‹
    # ==============================================================
    print("ğŸ“‹ æ­£åœ¨è¯†åˆ«æŠ¥å‘Šç±»å‹...")
    
    classifier_prompt = REPORT_CLASSIFIER_PROMPT.format(ocr_text=ocr)
    try:
        completion = qwen.chat.completions.create(
            model=medical_model,
            messages=[
                {'role': 'system', 'content': SYSTEM_PROMPT},
                {'role': 'user', 'content': classifier_prompt}
            ]
        )
        classifier_text = completion.choices[0].message.content
        # print(f"åˆ†ç±»ç»“æœåŸæ–‡: {classifier_text}")
        
        # ğŸ” è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºåˆ†ç±»å™¨åŸå§‹å“åº”
        print(f"ğŸ” åˆ†ç±»å™¨åŸå§‹å“åº”: {classifier_text}")
        
        classification_result = safe_json_load(classifier_text)
        if classification_result and 'report_type' in classification_result:
            report_type = classification_result['report_type']
            confidence = classification_result.get('confidence', 'æœªçŸ¥')
            reason = classification_result.get('reason', 'æ— ç†ç”±')
            print(f"âœ… æŠ¥å‘Šç±»å‹è¯†åˆ«å®Œæˆ: {report_type} (ç½®ä¿¡åº¦: {confidence})")
            print(f"   åˆ¤æ–­ç†ç”±: {reason}")
        else:
            print("âš ï¸ æŠ¥å‘Šç±»å‹è¯†åˆ«å¤±è´¥ï¼Œé»˜è®¤æŒ‰CTAå¤„ç†")
            print(f"ğŸ” åˆ†ç±»ç»“æœè§£æå¤±è´¥ï¼ŒåŸå§‹å†…å®¹: {classification_result}")
            report_type = "CTA"
    except Exception as e:
        print(f"âŒ æŠ¥å‘Šç±»å‹è¯†åˆ«å‡ºé”™: {e}ï¼Œé»˜è®¤æŒ‰CTAå¤„ç†")
        report_type = "CTA"
    
    # åˆå§‹åŒ–top_data
    top_data = {}
    
    # ğŸ” è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºåˆå§‹è¡¨æ ¼çŠ¶æ€
    print(f"ğŸ” åˆå§‹formatted_tableè¡Œæ•°: {len(formatted_table)}")
    if len(formatted_table) > 0:
        print(f"ğŸ” åˆå§‹è¡¨æ ¼å‰5è¡Œåç§°: {formatted_table['åç§°'].head().tolist()}")
    
    # ==============================================================
    # ç¬¬äºŒæ­¥ï¼šæ ¹æ®æŠ¥å‘Šç±»å‹æ‰§è¡Œä¸åŒçš„å¤„ç†é€»è¾‘
    # ==============================================================
    
    print(f"ğŸ” å³å°†è¿›å…¥å¤„ç†åˆ†æ”¯: {report_type}")
    
    if report_type == "CTA":
        print("ğŸ” æŒ‰å† è„‰CTAæŠ¥å‘Šå¤„ç†...")
        
        # CTAæŠ¥å‘Šçš„é¡¶éƒ¨ä¿¡æ¯æå–
        header_data = [["å† çŠ¶åŠ¨è„‰é’™åŒ–æ€»ç§¯åˆ†", "LM", "LAD", "LCX", "RCA"]]
        
        # æå–é¡¶éƒ¨åŸºæœ¬ä¿¡æ¯
        for row in header_data:
            input_prompt = FILL_IN_FORM_PROMPT.format(ocr_text=ocr, key_info=row)
            try:
                completion = qwen.chat.completions.create(
                    model=medical_model,
                    messages=[
                        {'role': 'system', 'content': SYSTEM_PROMPT},
                        {'role': 'user', 'content': input_prompt}
                    ]
                )
                text = completion.choices[0].message.content
                tmp = safe_json_load(text)
                if tmp is not None:
                    for k in tmp:
                        top_data[k] = tmp[k] if tmp[k] != "NO" else ""
            except Exception as e:
                print(f"âš ï¸ CTAé¡¶éƒ¨ä¿¡æ¯æå–å¤±è´¥: {e}")
        
        # æå–CTAä¸“ç”¨çš„åˆ†ç±»ä¿¡æ¯
        cta_prompts = [
            FILLIN_PROMPT_2.format(ocr_text=ocr),  # å† çŠ¶åŠ¨è„‰èµ·æºã€èµ°å½¢åŠç»ˆæ­¢
            FILLIN_PROMPT_3.format(ocr_text=ocr),  # å† è„‰ä¼˜åŠ¿å‹
            FILLIN_PROMPT_4.format(ocr_text=ocr)   # å¼‚å¸¸æè¿°
        ]
        
        for input_prompt in cta_prompts:
            try:
                completion = qwen.chat.completions.create(
                    model=medical_model,
                    messages=[
                        {'role': 'system', 'content': SYSTEM_PROMPT},
                        {'role': 'user', 'content': input_prompt}
                    ]
                )
                text = completion.choices[0].message.content
                tmp = safe_json_load(text)
                if tmp is not None and 'key_name' in tmp and 'result' in tmp:
                    top_data[tmp['key_name']] = tmp['result']
            except Exception as e:
                print(f"âš ï¸ CTAåˆ†ç±»ä¿¡æ¯æå–å¤±è´¥: {e}")

        # å¹¶è¡Œå¤„ç†CTAçš„55ä¸ªå† è„‰èŠ‚æ®µ
        print("ğŸ”„ å¼€å§‹å¹¶è¡Œå¤„ç†å† è„‰èŠ‚æ®µ...")

        import concurrent.futures
        from functools import partial
        
        def process_location(location, ocr, qwen, row_index, system_prompt, model_name):
            """å¤„ç†å•ä¸ªå† è„‰èŠ‚æ®µçš„å‡½æ•°ï¼Œç”¨äºå¹¶è¡Œæ‰§è¡Œ"""
            if location not in row_index:
                return None, None, None
            
            ridx = row_index[location]
            
            input_prompt = FILLIN_PROMPT_5.format(ocr_text=ocr, location=location)
            
            # å®ç°é‡è¯•æœºåˆ¶ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
            max_retries = 3
            attempt = 0
            
            while attempt < max_retries:
                try:
                    completion = qwen.chat.completions.create(
                        model=model_name,
                        messages=[
                            {'role': 'system', 'content': system_prompt},
                            {'role': 'user', 'content': input_prompt}
                        ]
                    )
                    text = completion.choices[0].message.content
                    tmp = safe_json_load(text)
                    return location, ridx, tmp
                except Exception as e:
                    attempt += 1
                    if attempt < max_retries:
                        # æŒ‡æ•°é€€é¿ï¼šç­‰å¾…æ—¶é—´ä¸º 2^attempt ç§’ï¼Œæœ€å¤§ç­‰å¾…32ç§’
                        wait_time = min(2 ** attempt, 32)
                        import time
                        time.sleep(wait_time)
                    else:
                        print(f"âŒ {location} å¤„ç†å¤±è´¥: {e}")
                        return location, ridx, None
            
            return location, ridx, None
        
        # è·å–æ‰€æœ‰éœ€è¦å¤„ç†çš„ä½ç½®
        locations_to_process = []
        for i in range(len(formatted_table)):
            location = formatted_table.iloc[i]["åç§°"]
            if location in row_index:
                locations_to_process.append(location)
        
        # å¹¶è¡Œå¤„ç†
        process_func = partial(process_location, ocr=ocr, qwen=qwen, row_index=row_index, system_prompt=SYSTEM_PROMPT, model_name=medical_model)
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
            results = list(executor.map(process_func, locations_to_process))
        
        # æ›´æ–°è¡¨æ ¼ - åªæ›´æ–°CTAç›¸å…³çš„å­—æ®µ
        updatable_columns = ["ç±»å‹", "ç—‡çŠ¶", "æ•°å€¼", "å•ä½"]
        all_extracted_data = {}  # æ”¶é›†æ‰€æœ‰æå–åˆ°çš„æ•°æ®ç”¨äºåç»­åŠ¨æ€æ·»åŠ 
        
        for location, ridx, tmp in results:
            if tmp is not None:
                # æ£€æŸ¥æ˜¯å¦æœ‰å®é™…æ•°æ®ï¼ˆä¸æ˜¯é»˜è®¤çš„"-"å€¼ï¼‰
                has_real_data = False
                for col in updatable_columns:
                    value = tmp.get(col, "")
                    if value == "NO":
                        value = ""
                    if value and value != "-":
                        formatted_table.at[ridx, col] = value
                        if col == "æ•°å€¼" and value != "-":
                            has_real_data = True
                            all_extracted_data[location] = value
                
                if has_real_data:
                    print(f"âœ… CTAåŒ¹é…æˆåŠŸ: {location}")
        
        # æ”¶é›†æ‰€æœ‰å¯èƒ½è¢«é—æ¼çš„CTAæ•°æ®ï¼Œæ·»åŠ åˆ°è¡¨æ ¼åº•éƒ¨
        print("ğŸ“Š æ£€æŸ¥æ˜¯å¦æœ‰é—æ¼çš„CTAæ•°æ®...")
        try:
            # ä½¿ç”¨é€šç”¨æå–promptæ‰¾å‡ºå¯èƒ½é—æ¼çš„å† è„‰ç›¸å…³æ•°æ®
            cta_general_prompt = f"""
æˆ‘å°†ç»™ä½ ä¸€æ®µå† è„‰CTAè¯Šæ–­æŠ¥å‘Šç»è¿‡OCRæå–ä¹‹åçš„æ–‡æœ¬ã€‚è¯·ä»ä¸­æå–æ‰€æœ‰å…·ä½“çš„å† è„‰èŠ‚æ®µä¿¡æ¯å’Œæµ‹é‡æ•°å€¼ï¼Œç‰¹åˆ«æ˜¯é‚£äº›å¯èƒ½æ²¡æœ‰åŒ…å«åœ¨ä»¥ä¸‹å·²çŸ¥é¡¹ç›®ä¸­çš„æ•°æ®ï¼š

**å·²çŸ¥é¡¹ç›®ï¼š**
{', '.join(all_extracted_data.keys())}

**åŒ»ç–—è¯Šæ–­æŠ¥å‘Šï¼š**
{ocr}

**æå–è§„åˆ™ï¼š**
1. é‡ç‚¹å…³æ³¨å† è„‰èŠ‚æ®µï¼ˆå¦‚"å·¦ä¸»å¹²"ã€"å‰é™æ”¯"ã€"å›æ—‹æ”¯"ã€"å³å† "çš„å„ä¸ªåˆ†æ®µï¼‰
2. æå–æ–‘å—ã€ç‹­çª„ã€é’™åŒ–ç­‰ç›¸å…³ä¿¡æ¯
3. åŒ…å«å…·ä½“çš„ç‹­çª„ç¨‹åº¦æ•°å€¼

**è¿”å›æ ¼å¼ï¼š**
è¯·ä»¥JSONæ ¼å¼è¿”å›æ‰€æœ‰å† è„‰ç›¸å…³æ•°æ®ï¼š
{{
"å† è„‰èŠ‚æ®µæˆ–æµ‹é‡é¡¹ç›®": "ç›¸å…³ä¿¡æ¯æˆ–æ•°å€¼",
"å† è„‰èŠ‚æ®µæˆ–æµ‹é‡é¡¹ç›®": "ç›¸å…³ä¿¡æ¯æˆ–æ•°å€¼",
...
}}

**åªè¿”å›JSONï¼Œä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹ã€‚**
"""
            
            completion = qwen.chat.completions.create(
                model=medical_model,
                messages=[
                    {'role': 'system', 'content': SYSTEM_PROMPT},
                    {'role': 'user', 'content': cta_general_prompt}
                ]
            )
            cta_data_text = completion.choices[0].message.content
            cta_data = safe_json_load(cta_data_text)
            
            if cta_data:
                # æ‰¾å‡ºæœªåŒ¹é…çš„æ•°æ®
                unmatched_data = []
                for key, value in cta_data.items():
                    # æ£€æŸ¥è¿™ä¸ªæ•°æ®æ˜¯å¦å·²ç»è¢«åŒ¹é…è¿‡
                    was_matched = any(str(all_extracted_data.get(existing_key, "")) == str(value) 
                                    for existing_key in all_extracted_data.keys())
                    
                    # åŒæ—¶æ£€æŸ¥keyæ˜¯å¦å·²ç»åœ¨ç°æœ‰è¡¨æ ¼çš„åç§°åˆ—ä¸­
                    key_exists = any(key in str(formatted_table.iloc[i]["åç§°"]) or 
                                   str(formatted_table.iloc[i]["åç§°"]) in key 
                                   for i in range(len(formatted_table)))
                    
                    if not was_matched and not key_exists and value and value != "-":
                        unmatched_data.append((key, value))
                
                # å°†æœªåŒ¹é…çš„æ•°æ®æ·»åŠ åˆ°è¡¨æ ¼åº•éƒ¨
                if unmatched_data:
                    print(f"ğŸ“‹ å‘ç° {len(unmatched_data)} ä¸ªæœªåŒ¹é…çš„CTAæ•°æ®ï¼Œæ·»åŠ åˆ°è¡¨æ ¼åº•éƒ¨...")
                    
                    import pandas as pd
                    for key, value in unmatched_data:
                        # åˆ†ç¦»æ•°å€¼å’Œå•ä½
                        pure_value, extracted_unit = separate_value_and_unit(str(value))
                        
                        new_row = {
                            "åç§°": key,
                            "è‹±æ–‡": "",  # ä¸æ¨æµ‹è‹±æ–‡ï¼Œä¿æŒç©ºç™½
                            "æ–‘å—ç§ç±»": "",
                            "ç±»å‹": "",
                            "ç—‡çŠ¶": "",
                            "æ•°å€¼": pure_value,
                            "å•ä½": extracted_unit,  # è‡ªåŠ¨æå–çš„å•ä½
                            "ç‹­çª„ç¨‹åº¦": "",
                            "é—­å¡": ""
                        }
                        
                        # ä½¿ç”¨pd.concatæ·»åŠ æ–°è¡Œåˆ°è¡¨æ ¼æœ€å‰é¢
                        new_row_df = pd.DataFrame([new_row])
                        formatted_table = pd.concat([new_row_df, formatted_table], ignore_index=True)
                        
                        print(f"â• æ·»åŠ æ–°è¡Œ: {key} = {value}")
                        
        except Exception as e:
            print(f"âš ï¸ CTAè¡¥å……æå–å¤±è´¥: {e}")
        
        print("âœ… CTAæŠ¥å‘Šå¤„ç†å®Œæˆ")
    
    elif report_type == "Ultrasound":
        print("ğŸ«€ æŒ‰å¿ƒè„è¶…å£°æŠ¥å‘Šå¤„ç†...")
        
        # å…ˆæŠ½å–å¤´éƒ¨ä¿¡æ¯ï¼ˆå§“å/æ€§åˆ«/å¹´é¾„/è®¾å¤‡/æ‰€è§/æç¤ºç­‰ï¼‰ï¼Œç¼ºå¤±ç•™ç©º
        try:
            from medical_agent.prompts import ULTRASOUND_HEADER_PROMPT
            header_prompt = ULTRASOUND_HEADER_PROMPT.format(ocr_text=ocr)
            completion = qwen.chat.completions.create(
                model=medical_model,
                messages=[
                    {'role': 'system', 'content': SYSTEM_PROMPT},
                    {'role': 'user', 'content': header_prompt}
                ]
            )
            text = completion.choices[0].message.content
            header_json = safe_json_load(text) or {}
            if isinstance(header_json, dict):
                for k, v in header_json.items():
                    top_data[k] = v or ""
        except Exception as e:
            print(f"âš ï¸ è¶…å£°å¤´éƒ¨ä¿¡æ¯æå–å¤±è´¥: {e}")
        
        # å…³é”®æµ‹é‡å€¼ä¸å†åœ¨æ­¤å¤„ç”¨LLMæŠ½å–ï¼Œæ”¹ä¸ºåœ¨è¡¨æ ¼å®Œæˆåä»è¡¨æ ¼ä¸­å›å¡«åˆ° top_data
        # ï¼ˆLVEF, LVEDD, LVESD, IVSd, LVPWd, E/A, eâ€², aâ€²ï¼‰

        # ä¿ç•™åŸæœ‰â€œå¼‚å¸¸æè¿°â€æå–ä½œä¸ºè¡¥å……ï¼ˆè‹¥ header_json æœªç»™åˆ°ï¼‰
        try:
            if not top_data.get('å¼‚å¸¸æè¿°'):
                input_4 = FILLIN_PROMPT_4.format(ocr_text=ocr)
                completion = qwen.chat.completions.create(
                    model=medical_model,
                    messages=[
                        {'role': 'system', 'content': SYSTEM_PROMPT},
                        {'role': 'user', 'content': input_4}
                    ]
                )
                text = completion.choices[0].message.content
                tmp = safe_json_load(text)
                if tmp is not None and 'key_name' in tmp and 'result' in tmp:
                    top_data[tmp['key_name']] = tmp['result']
        except Exception as e:
            print(f"âš ï¸ è¶…å£°å¼‚å¸¸æè¿°æå–å¤±è´¥: {e}")
        
        # å¹¶è¡Œå¤„ç†è¶…å£°çš„55ä¸ªæµ‹é‡é¡¹ç›®
        print("ğŸ”„ å¼€å§‹å¹¶è¡Œå¤„ç†è¶…å£°æµ‹é‡é¡¹ç›®...")
        
        import concurrent.futures
        from functools import partial
        
        # å…ˆä»å…¨æ–‡æŠ½å–æ‰€æœ‰å€™é€‰â€œé¡¹ç›®â†’æ•°å€¼(å¯å«å•ä½)â€
        candidates = {}
        try:
            from medical_agent.prompts import ULTRASOUND_ALL_MEASUREMENTS_PROMPT
            cand_prompt = ULTRASOUND_ALL_MEASUREMENTS_PROMPT.format(ocr_text=ocr)
            completion = qwen.chat.completions.create(
                model=medical_model,
                messages=[
                    {'role': 'system', 'content': SYSTEM_PROMPT},
                    {'role': 'user', 'content': cand_prompt}
                ]
            )
            cand_text = completion.choices[0].message.content
            cand_json = safe_json_load(cand_text) or {}
            if isinstance(cand_json, dict):
                candidates = cand_json
        except Exception as e:
            print(f"âš ï¸ å€™é€‰æµ‹é‡æŠ½å–å¤±è´¥: {e}")
        
        # å‡†å¤‡æ ‡å‡†è¡¨å€™é€‰é›†åˆï¼ˆåç§°ä¸è‹±æ–‡ï¼‰
        std_name_to_ridx = {}
        std_choices = []
        for i in range(len(formatted_table)):
            cn_name = str(formatted_table.iloc[i]["åç§°"]).strip()
            std_name_to_ridx[cn_name.lower()] = i
            std_choices.append(cn_name)
            eng = str(formatted_table.iloc[i].get("è‹±æ–‡", "") or "").strip()
            if eng:
                std_name_to_ridx[eng.lower()] = i
                std_choices.append(eng)
        
        # çŸ¥è¯†åº“ç´¢å¼•
        try:
            from medical_agent.normalizer import _load_kb, _build_alias_index
            kb = _load_kb()
            alias_to_canonical, canonical_meta = _build_alias_index(kb)
        except Exception as _e:
            alias_to_canonical, canonical_meta = {}, {}
        
        free_rows = []
        consumed_keys = set()
        llm_cache: Dict[str, str] = {}
        
        def update_std_row_by_ridx(ridx: int, raw_value: str):
            pure_value, extracted_unit = separate_value_and_unit(str(raw_value))
            if pure_value and pure_value != "-":
                formatted_table.at[ridx, "æ•°å€¼"] = pure_value
                if extracted_unit:
                    cur_unit = str(formatted_table.at[ridx, "å•ä½"]) if formatted_table.at[ridx, "å•ä½"] is not None else ""
                    if not cur_unit.strip():
                        formatted_table.at[ridx, "å•ä½"] = extracted_unit
        
        # é˜¶æ®µ1ï¼šæ ‡å‡†è¡¨ï¼ˆç²¾ç¡® -> rapidfuzz -> ç°åŒºQwenæ ¡éªŒï¼‰
        for key, raw_value in candidates.items():
            q = _preclean_name(key)
            # exact (å¤§å°å†™ä¸æ•æ„Ÿ)
            if q.lower() in std_name_to_ridx:
                update_std_row_by_ridx(std_name_to_ridx[q.lower()], raw_value)
                consumed_keys.add(key)
                continue
            # rapidfuzz
            best = process.extractOne(q, std_choices, scorer=fuzz.WRatio)
            if best:
                choice, score, _ = best
                if score >= 95:
                    ridx = std_name_to_ridx.get(choice.lower())
                    if ridx is not None:
                        update_std_row_by_ridx(ridx, raw_value)
                        consumed_keys.add(key)
                        continue
                elif 80 <= score < 95:
                    # ç°åŒºï¼šå–topKå€™é€‰äº¤ç»™Qwenå¤æ ¸
                    topk = _rapid_topk(q, std_choices, k=8)
                    cands = []
                    for c, s, _ in topk:
                        ridx = std_name_to_ridx.get(c.lower())
                        if ridx is None:
                            continue
                        eng = str(formatted_table.iloc[ridx].get("è‹±æ–‡", "") or "").strip()
                        cands.append({"exact_name": str(formatted_table.iloc[ridx]["åç§°"]).strip(), "aliases": [eng] if eng else []})
                    cache_key = f"std::{q}::{json.dumps(cands, ensure_ascii=False)}"
                    match = llm_cache.get(cache_key)
                    if match is None:
                        match = _ask_qwen_alias(qwen, medical_model, key, cands)
                        llm_cache[cache_key] = match or ""
                    if match and match != "no_match":
                        ridx = std_name_to_ridx.get(match.lower())
                        # è‹¥ç›´æ¥ä¸­æ–‡åæœªå‘½ä¸­ï¼Œå†éå†æ‰¾åç§°åŒ¹é…
                        if ridx is None:
                            for i in range(len(formatted_table)):
                                if str(formatted_table.iloc[i]["åç§°"]).strip() == match:
                                    ridx = i
                                    break
                        if ridx is not None:
                            update_std_row_by_ridx(ridx, raw_value)
                            consumed_keys.add(key)
                            continue
        
        # é˜¶æ®µ2ï¼šKBï¼ˆrapidfuzz -> ç°åŒºQwenæ ¡éªŒï¼‰ -> è‡ªç”±è¡Œ
        kb_alias_keys = list(alias_to_canonical.keys()) if alias_to_canonical else []
        for key, raw_value in candidates.items():
            if key in consumed_keys:
                continue
            q = _preclean_name(key).lower()
            canonical = ""
            if q in alias_to_canonical:
                canonical = alias_to_canonical[q]
            elif kb_alias_keys:
                best = process.extractOne(q, kb_alias_keys, scorer=fuzz.WRatio)
                if best:
                    cand_key, score, _ = best
                    if score >= 90:
                        canonical = alias_to_canonical[cand_key]
                    elif 80 <= score < 90:
                        # ç°åŒºï¼šå–topKå€™é€‰äº¤ç»™Qwen
                        topk = process.extract(q, kb_alias_keys, scorer=fuzz.WRatio, limit=8)
                        # å½’å¹¶æˆ canonical å€™é€‰å¹¶å»é‡
                        canon_set = []
                        seen = set()
                        for ck, s, _ in topk:
                            cn = alias_to_canonical.get(ck, "")
                            if cn and cn not in seen:
                                meta = canonical_meta.get(cn, {})
                                aliases = []
                                abbr = str(meta.get("æµ‹é‡å€¼ç®€å†™", "") or "").strip()
                                eng = str(meta.get("æµ‹é‡å€¼è‹±æ–‡", "") or "").strip()
                                if abbr: aliases.append(abbr)
                                if eng: aliases.append(eng)
                                alias_field = meta.get("åˆ«å", []) or []
                                if isinstance(alias_field, str):
                                    alias_field = [a.strip() for a in alias_field.split(";") if a.strip()]
                                aliases.extend(alias_field)
                                canon_set.append({"exact_name": cn, "aliases": aliases})
                                seen.add(cn)
                        cache_key = f"kb::{q}::{json.dumps(canon_set, ensure_ascii=False)}"
                        match = llm_cache.get(cache_key)
                        if match is None:
                            match = _ask_qwen_alias(qwen, medical_model, key, canon_set)
                            llm_cache[cache_key] = match or ""
                        if match and match != "no_match":
                            canonical = match
            
            if canonical:
                meta = canonical_meta.get(canonical, {})
                abbr = str(meta.get("æµ‹é‡å€¼ç®€å†™", "") or "").strip()
                english = str(meta.get("æµ‹é‡å€¼è‹±æ–‡", "") or "").strip()
                unit_std = str(meta.get("å•ä½", "") or "").strip()
                pure_value, extracted_unit = separate_value_and_unit(str(raw_value))
                final_unit = extracted_unit or unit_std
                std_name = f"{canonical}({abbr})" if abbr else canonical
                free_rows.append({
                    "åç§°": std_name,
                    "è‹±æ–‡": english,
                            "ç±»å‹": "",
                            "ç—‡çŠ¶": "",
                            "æ•°å€¼": pure_value,
                    "å•ä½": final_unit
                })
                consumed_keys.add(key)
        
        # é˜¶æ®µ3ï¼šå…œåº•è‡ªç”±è¡Œ
        for key, raw_value in candidates.items():
            if key in consumed_keys:
                continue
            pure_value, extracted_unit = separate_value_and_unit(str(raw_value))
            free_rows.append({
                "åç§°": str(key).strip(),
                "è‹±æ–‡": "",
                "ç±»å‹": "",
                "ç—‡çŠ¶": "",
                "æ•°å€¼": pure_value,
                "å•ä½": extracted_unit
            })
        
        # å°†è‡ªç”±è¡Œæ’å…¥åˆ°è¡¨æ ¼å‰éƒ¨
        if free_rows:
            import pandas as pd
            free_df = pd.DataFrame(free_rows)
            formatted_table = pd.concat([free_df, formatted_table], ignore_index=True)
        
        print("âœ… è¶…å£°æŠ¥å‘Šå¤„ç†å®Œæˆ")
    
    else:
        print(f"âš ï¸ æœªçŸ¥æŠ¥å‘Šç±»å‹: {report_type}ï¼Œè·³è¿‡å¤„ç†")
    
    # ==============================================================
    # ç¬¬ä¸‰æ­¥ï¼šæ›´æ–°çŠ¶æ€ä¸­çš„è¡¨æ ¼å¹¶ä¿å­˜ç»“æœ
    # ==============================================================
    
    # å¦‚æœçŠ¶æ€ä¸­æœ‰å¤„ç†æ—¶é—´çš„èµ·ç‚¹ï¼Œæ‰“å°å¤„ç†æ—¶é—´
    if 'process_start_time' in state['context']:
        end_timer_and_print(state['context']['process_start_time'], 
                          state['context'].get('current_file_name', 'unknown'), 
                          state['context'].get('file_type', 'file'))
    
    print("ğŸ’¾ ä¿å­˜ç»“æœ...")
    
    # æ›´æ–°çŠ¶æ€ä¸­çš„è¡¨æ ¼ï¼ˆå¯èƒ½å·²ç»è¢«åŠ¨æ€æ‰©å±•ï¼‰
    try:
        # åœ¨ä¿å­˜å‰è¿›è¡ŒåŸºäºçŸ¥è¯†åº“çš„å½’ä¸€åŒ–
        from medical_agent.normalizer import normalize_table_with_kb
        formatted_table = normalize_table_with_kb(formatted_table)
    except Exception as _e:
        # å½’ä¸€åŒ–å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
        print(f"âš ï¸ å½’ä¸€åŒ–æ­¥éª¤è·³è¿‡: {_e}")
    state['formatted_table'] = formatted_table
    
    # ä»æœ€ç»ˆè¡¨æ ¼ä¸­å›å¡«å…³é”®æµ‹é‡å€¼åˆ° top_dataï¼ˆé€ä¸ªå‡»ç ´ï¼šæ¯ä¸ª key å•ç‹¬è¯¢é—® LLM â†’ å†ä»è¡¨å†…å–å€¼ï¼‰
    try:
        from medical_agent.prompts import ULTRASOUND_KEY_NAME_PICK_PROMPT
        import json as _json

        TARGET_KEYS = ["LVEF", "LVEDD", "LVESD", "IVSd", "LVPWd", "E/A", "eâ€²", "E/eâ€²", "aâ€²"]
        # å€™é€‰åç§°ï¼šä»…å–â€œåç§°â€åˆ—çš„éç©ºå»é‡å€¼
        candidate_names = []
        seen = set()
        for i in range(len(formatted_table)):
            nm = str(formatted_table.iloc[i].get("åç§°", "") or "").strip()
            if nm and nm not in seen:
                candidate_names.append(nm)
                seen.add(nm)
        # å¿«é€Ÿç´¢å¼•ï¼šåç§° -> å€¼å­—ç¬¦ä¸²
        name_to_val = {}
        for i in range(len(formatted_table)):
            nm = str(formatted_table.iloc[i].get("åç§°", "") or "").strip()
            val = str(formatted_table.iloc[i].get("æ•°å€¼", "") or "").strip()
            unit = str(formatted_table.iloc[i].get("å•ä½", "") or "").strip()
            if nm and val and val not in ("-", "NO"):
                name_to_val[nm] = f"{val}{unit}" if unit else val

        for key in TARGET_KEYS:
            try:
                payload = ULTRASOUND_KEY_NAME_PICK_PROMPT.format(
                    target_key=key,
                    candidate_names_json=_json.dumps(candidate_names, ensure_ascii=False)
                )
                completion = qwen.chat.completions.create(
                    model=medical_model,
                    messages=[
                        {'role': 'system', 'content': SYSTEM_PROMPT},
                        {'role': 'user', 'content': payload}
                    ]
                )
                text = completion.choices[0].message.content
                res = safe_json_load(text) or {}
                match_name = ""
                if isinstance(res, dict):
                    match_name = str(res.get("match", "") or "").strip()
                if match_name:
                    top_data[key] = name_to_val.get(match_name, "")
                else:
                    # æ— åŒ¹é…åˆ™ç½®ç©ºï¼ˆç¡®ä¿ä¸ä¿ç•™æ—§å€¼ï¼‰
                    top_data[key] = ""
            except Exception as _inner:
                print(f"âš ï¸ å…³é”®é¡¹ {key} å›å¡«å¤±è´¥: {_inner}")
                top_data[key] = ""
    except Exception as _e:
        print(f"âš ï¸ é¡¶éƒ¨å…³é”®æµ‹é‡å€¼å›å¡«å¤±è´¥: {_e}")

    save_df_to_cache(formatted_table, "qwen_cache")

    # åŠ è½½å¹¶æ˜¾ç¤ºç»“æœ
    df = load_df_from_cache("qwen_cache")
    state['context']['df'] = df
    
    print("ğŸ¯ æ˜¾ç¤ºç»“æœ...")
    show_popup_with_df(df, top_data)

    print("âœ¨ æ™ºèƒ½ç»“æ„åŒ–æå–å®Œæˆï¼")
    return state


            

# Define the response generation node
def create_response_node(state: AgentState):
    """Create a node for handling user input and generating responses."""
    # Get the last user message
    last_message = next((m for m in reversed(state["messages"]) if m["role"] == "user"), None)
    
    if not last_message:
        return state
        
    # Get content and content type
    content = last_message["content"]
    content_type = last_message.get("content_type", "text")
    
    # Get response from LLM using state's llm
    # result = state["gpt"].invoke([system_message, user_message])
    completion = call_qwen_vl_api(state)
    
    # Add the response to messages
    state["messages"].append({
        "role": "assistant",
        "content": completion,
        "content_type": "text"
    })
    
    return state
    
def show_results(state: AgentState):
    """Show the results of the agent's response."""
    # Get the last assistant message
    last_assistant_message = next((m for m in reversed(state["messages"]) if m["role"] == "assistant"), None)

    if last_assistant_message:
        print("Agent Response:")
        print(last_assistant_message["content"])
    else:
        print("No response from the agent.")
    return state

def _preclean_name(text: str) -> str:
    import re
    if not text:
        return ""
    s = str(text)
    s = re.sub(r"[\(ï¼ˆ][^\)ï¼‰]*[\)ï¼‰]", "", s)
    s = re.sub(r"[%ï¼š:ï¼Œ,ã€‚Â·/\\]+", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s


def _rapid_topk(query: str, choices: List[str], k: int = 10) -> List[tuple]:
    if not choices:
        return []
    return process.extract(_preclean_name(query), choices, scorer=fuzz.WRatio, limit=k)


def _ask_qwen_alias(qwen_client, model_name: str, query: str, candidates: List[Dict[str, Any]]) -> str:
    from medical_agent.prompts import ALIAS_VALIDATION_PROMPT
    try:
        payload = ALIAS_VALIDATION_PROMPT.format(
            query=query,
            candidates_json=json.dumps(candidates, ensure_ascii=False)
        )
        completion = qwen_client.chat.completions.create(
            model=model_name,
            messages=[
                {'role': 'system', 'content': SYSTEM_PROMPT},
                {'role': 'user', 'content': payload}
            ]
        )
        text = completion.choices[0].message.content
        res = safe_json_load(text) or {}
        match = res.get("match", "") if isinstance(res, dict) else ""
        return str(match)
    except Exception:
        return ""

# Build the complete agent
def build_medical_agent():
    """Build a simplified medical agent with a single node."""
    
    graph = StateGraph(AgentState)
    
    graph.add_node("init_llms", init_llms)
    graph.add_node("input_node", create_input_node)
    graph.add_node("ocr_node", ocr_node)
    graph.add_node("response_node", create_response_node)
    graph.add_node("show_results", show_results)
    graph.add_node("fill_form_node", fill_form_node)
    # Set the entry point and edge
    graph.set_entry_point("init_llms")
    graph.add_edge("init_llms", "input_node")
    graph.add_edge("input_node", "ocr_node")
    graph.add_edge("ocr_node", "fill_form_node")
    # graph.add_edge("ocr_node", "response_node")
    # graph.add_edge("response_node", "show_results")
    # Compile the graph
    return graph.compile()